# Orchestrate Azure Databricks jobs with Apache Airflow

## üí°Objetivos
Projeto visa fornecer suporte √† empresas que importam produtos de diferentes pa√≠ses, ajudando-a a ter uma previs√£o mais precisa dos gastos. Para isso, √© essencial acompanhar as flutua√ß√µes das moedas relevantes para a empresa, incluindo o D√≥lar Americano (USD), Libra Esterlina (GBP) e Euro (EUR). Nesse contexto, prop√µe-se o desenvolvimento de uma solu√ß√£o capaz de extrair diariamente as cota√ß√µes dessas moedas, consolidando-as em uma tabela e em gr√°ficos de f√°cil interpreta√ß√£o.

Para atingir esse objetivo, o projeto realizar√° a constru√ß√£o e execu√ß√£o de um pipeline completo, utilizando as capacidades do Apache Airflow e do Databricks na nuvem Azure. A integra√ß√£o com a plataforma de comunica√ß√£o interna Slack ser√° aproveitada para o envio autom√°tico da tabela e dos gr√°ficos, garantindo uma comunica√ß√£o eficiente e centralizada dentro da equipe.

###### Imagem 1: Grid View - Visualiza√ß√£o da execu√ß√£o do DAG
<img src="/img/001-airflow-grid">


## üñ•Ô∏èDesafios do Projeto
Fazer o uso do Databricks em conjunto com o Spark e a API do Pandas Spark para efetuar a leitura, manipula√ß√£o e extra√ß√£o di√°ria dos dados de cota√ß√£o das moedas, cria√ß√£o de fluxos de trabalho (Workflows), implementado um job individual para cada notebook. O Apache Airflow para orquestrar e agendar a execu√ß√£o desses notebooks no ambiente do Azure Databricks. Esta abordagem integrada aproveita as potentes funcionalidades de ferramentas como PySpark, Airflow, Databricks e as requisi√ß√µes de API de taxa de c√¢mbio, como a "Exchange Rates Data API - APILayer". E por √∫ltimo, criar um bot do Slack, utilizando sua API, para receber dados com a tabela de c√¢mbio e gr√°ficos do Databricks/Airflow.

###### Imagem 2: Fluxo de trabalho do Databricks
<img src="/img/002-workflow-databricks.png">


## üìÑConhecimentos Desenvolvidos
|Atividades|Realizadas |
|----------|-----------|
| Identificar como instalar o Airflow | Compreender e acessar o Azure Databricks |
| Reconhecer e controlar os gastos da Azure | Identificar e acessar a API ‚ÄúExchange Rates Data API‚Äù | 
| Fazer a requisi√ß√£o dos dados para a API | Organizar esquematizar o projeto conforme a arquitetura medalh√£o | 
| Reconhecer e salvar os dados no formato Parquet | Orquestrar o notebook no Databricks com o Airflow | 
| Juntar os dados extra√≠dos em somente um PySpark DataFrame | Filtrar os dados requeridos | 
| Transformar o DataFrame | Salvar os dados transformados no formato CSV | 
| Orquestrar com o Airflow os notebooks desenvolvidos no Databricks | Criar um Workspace no Slack | 
| Criar um Bot do Slack | Enviar arquivos CSV para o Workspace atrav√©s do bot do Slack | 
| Enviar as imagens para o Workspace atrav√©s do bot do Slack | Como trabalhar simultaneamente com o Airflow e o Databricks | 

##  üóÇÔ∏èOrganiza√ß√£o dos Arquivos

* notebooks: Arquivos desenvolvidos dentro do Databricks para manipula√ß√£o dos dados: extra√ß√£o, transforma√ß√£o e automatiza√ß√£o de relat√≥rios para envio no Slack
* dags: Arquivos respons√°veis por definir e executar Directed Acyclic Graphs (DAGs). Esses arquivos realizam um conjunto de tarefas para extrair dados do mercado financeiro sobre diferentes a√ß√µes da bolsa de valores.

##  üéûÔ∏èImagens do Projeto

###### Imagem 3: Microsoft Azure - Servi√ßos
<img src="/img/003-azure.png">

###### Imagem 4: Databricks Jobs
<img src="/img/004-job.png">

###### Imagem 5: Implementando um bot Slack para receber dados orquestrados atrav√©s da integra√ß√£o entre Databricks e Airflow.
<img src="/img/005-slack.png">


## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)